
from turtle import st
from langgraph.graph import StateGraph
from typing import List
from langgraph.graph import END
from langchain_core.messages import AIMessage, HumanMessage

from langgraph.graph import END
from typing import Dict, cast, Union
from langchain_core.messages import SystemMessage

from state import Graph_State       
from utils import init_generation_model, init_reflection_model
from logger import logger

generation_llm = init_generation_model()
reflection_llm = init_reflection_model()

async def generation_node(
    state: Graph_State
) -> Dict[str, str]:
    """Call the LLM powering our "agent".

    This function prepares the prompt, initializes the model, and processes the response.

    Args:
        state (State): The current state of the conversation.

    Returns:
        dict: A dictionary containing the model's response message.
    """
    # Format the system prompt. Customize this to change the agent's behavior.
    logger.info(f"Generation prompt: {state['generation_prompt']}")
    system_message = state["generation_prompt"]
    user_message = "here is the user's request: " + state["user_prompt"]
    if state["content"] != "":
        user_message += "\nhere is the content generated by the last run: " + state["content"]

    if state["reflecton_advice"] != "":
        user_message += "\nhere is the advice from the reflection agent: " + state["reflecton_advice"]

    if state["user_advice"] != "":
        user_message += "\nhere is the user's advice to improve the content: " + state["user_advice"]
    user_message += "\nplease generate the content based on the information provided."

    messages = [SystemMessage(content=system_message), HumanMessage(content=user_message)]

    # Get the model's response
    logger.info(f"Generation node - Sending messages to LLM: {messages}")
    response = cast(
        AIMessage,
        await generation_llm.ainvoke(
            messages
        ),
    )
    logger.info(f"Generation node - LLM response: {response.content}")

    # Return the model's response as a list to be added to existing messages
    return {"content": response.content}


async def reflection_node(
    state: Graph_State
) -> Dict[str, Union[str, int]]:
    """Call the LLM powering our "agent".

    This function prepares the prompt, initializes the model, and processes the response.
    Validate the quality of the generation agent's output.

    Args:
        state (State): The current state of the conversation.

    Returns:
        dict: A dictionary containing the model's response message.
    """
    # Format the system prompt. Customize this to change the agent's behavior.
    logger.info(f"Reflection prompt: {state['reflection_prompt']}")
    system_message = state["reflection_prompt"]
    
    user_message = "here is the user's request: " + state["user_prompt"]
    if state["content"] != "":
        user_message += "\nhere is the content generated by the last run: " + state["content"]

    if state["reflecton_advice"] != "":
        user_message += "\nhere is the advice from the reflection agent: " + state["reflecton_advice"]

    if state["user_advice"] != "":
        user_message += "\nhere is the user's advice to improve the content: " + state["user_advice"]
    user_message += "\nplease analyze and give advice on how to improve the content based on the information provided."

    messages = [SystemMessage(content=system_message), HumanMessage(content=user_message)]

    # Get the model's response
    logger.info(f"Reflection node - Sending messages to LLM: {messages}")
    response = cast(
        AIMessage,
        await reflection_llm.ainvoke(
            messages
        ),
    )
    logger.info(f"Reflection node - LLM response: {response.content}")

    # Return the model's response as a list to be added to existing messages
    return {"reflecton_advice": response.content, "reflect_count": state["reflect_count"] + 1}

builder = StateGraph(Graph_State)
builder.add_node("generation", generation_node)
builder.add_node("reflect", reflection_node)
builder.set_entry_point("generation")


def should_continue(state: Graph_State):
    logger.info(f"Should continue check - reflect_count: {state['reflect_count']}")
    if state["reflect_count"] > 1: # change reflect round setting here
        logger.info("Reflection count exceeded 1, ending graph execution")
        return END
    logger.info("Continuing to reflection node")
    return "reflect"


builder.add_conditional_edges("generation", should_continue)
builder.add_edge("reflect", "generation")
generation_graph_flow_v1 = builder.compile()